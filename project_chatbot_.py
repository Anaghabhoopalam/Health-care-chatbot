# -*- coding: utf-8 -*-
"""project chatbot .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wHrgWVktxN5NamVcjq_ptfn4V1EheZjG
"""

import json
import random

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

data_path ='/content/data/Intent.json'

with open(data_path, 'r') as f:
    data = json.load(f)

intents = data['intents']

X = []
y = []

for i in intents:
    for text in i['text']:
        X.append(text)
        y.append(i['intent'])

for i in range(10):
    print(f'text {X[i]} is labeled {y[i]}')

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)

X = tokenizer.texts_to_sequences(X)
X = pad_sequences(X)

num_classes = len(set(y))
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

model = Sequential([
    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=X.shape[1]),
    LSTM(128),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

EPOCHS = 20
BATCH_SIZE = 32
history = model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X, y))
model.save('/working/my_model_NLP.h5')

plt.figure(figsize=(12, 4))
epochs = range(1, EPOCHS + 1)

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(epochs, history.history['loss'], label='Training Loss', marker='o')
plt.plot(epochs, history.history['val_loss'], label='Validation Loss', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Loss (%)')
plt.grid()
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, history.history['accuracy'], label='Training Accuracy', marker='o')
plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy', marker='o')
plt.xlabel('Epochs')
print("plt.ylabel('Accuracy (%)')")
plt.grid()
plt.legend()

plt.tight_layout()
plt.show()

from tensorflow.keras.models import load_model

model = load_model('/working/my_model_NLP.h5')

random.shuffle(intents)

data_predict = intents[:10]

for idx in data_predict:
    texts = idx['text']
    intent = idx['intent']

    for text in texts:
        new_input_text = [text]
        new_input_sequences = tokenizer.texts_to_sequences(new_input_text)
        max_sequence_length = len(new_input_sequences)
        new_input_sequences = pad_sequences(new_input_sequences, maxlen=max_sequence_length)

        predictions = model.predict(new_input_sequences, verbose=0)

        predicted_labels = label_encoder.inverse_transform(predictions.argmax(axis=1))

        for pred in predicted_labels:
            if intent == pred:
                print(f'Your text: {text}')
                print('')
                print(f"ChatBot: {random.choice(idx['responses'])}")
                print('______________________________________________')